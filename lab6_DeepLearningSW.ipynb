{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY3H+Yq8EprDp8hCjWDzma",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hakmin1015/ArtificialIntelligence/blob/main/lab6_DeepLearningSW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24h3cVvtd6qF",
        "outputId": "6840da5d-c8be-4ef6-9827-44205e13e6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 4426013.0\n",
            "Epoch 10: Loss = 3481334.0\n",
            "Epoch 20: Loss = 2813196.5\n",
            "Epoch 30: Loss = 2327778.75\n",
            "Epoch 40: Loss = 1951291.25\n",
            "Epoch 50: Loss = 1646982.875\n",
            "Epoch 60: Loss = 1395568.5\n",
            "Epoch 70: Loss = 1184753.5\n",
            "Epoch 80: Loss = 1006107.25\n",
            "Epoch 90: Loss = 853805.0625\n",
            "Epoch 100: Loss = 723801.5\n",
            "Epoch 110: Loss = 612782.125\n",
            "Epoch 120: Loss = 518052.5\n",
            "Epoch 130: Loss = 437337.90625\n",
            "Epoch 140: Loss = 368638.34375\n",
            "Epoch 150: Loss = 310224.9375\n",
            "Epoch 160: Loss = 260649.125\n",
            "Epoch 170: Loss = 218637.59375\n",
            "Epoch 180: Loss = 183096.15625\n",
            "Epoch 190: Loss = 153072.09375\n",
            "Epoch 200: Loss = 127746.421875\n",
            "Epoch 210: Loss = 106419.359375\n",
            "Epoch 220: Loss = 88515.28125\n",
            "Epoch 230: Loss = 73508.1015625\n",
            "Epoch 240: Loss = 60953.74609375\n",
            "Epoch 250: Loss = 50468.890625\n",
            "Epoch 260: Loss = 41721.6015625\n",
            "Epoch 270: Loss = 34438.7421875\n",
            "Epoch 280: Loss = 28382.263671875\n",
            "Epoch 290: Loss = 23354.943359375\n",
            "Epoch 300: Loss = 19188.099609375\n",
            "Epoch 310: Loss = 15740.318359375\n",
            "Epoch 320: Loss = 12893.57421875\n",
            "Epoch 330: Loss = 10546.2802734375\n",
            "Epoch 340: Loss = 8614.3134765625\n",
            "Epoch 350: Loss = 7026.5029296875\n",
            "Epoch 360: Loss = 5723.71826171875\n",
            "Epoch 370: Loss = 4655.970703125\n",
            "Epoch 380: Loss = 3782.09814453125\n",
            "Epoch 390: Loss = 3067.87841796875\n",
            "Epoch 400: Loss = 2485.073486328125\n",
            "Epoch 410: Loss = 2010.4140625\n",
            "Epoch 420: Loss = 1624.299560546875\n",
            "Epoch 430: Loss = 1310.670166015625\n",
            "Epoch 440: Loss = 1056.29443359375\n",
            "Epoch 450: Loss = 850.2490234375\n",
            "Epoch 460: Loss = 683.534423828125\n",
            "Epoch 470: Loss = 548.8074951171875\n",
            "Epoch 480: Loss = 440.08660888671875\n",
            "Epoch 490: Loss = 352.4757080078125\n",
            "Epoch 500: Loss = 281.953125\n",
            "Epoch 510: Loss = 225.2580108642578\n",
            "Epoch 520: Loss = 179.74575805664062\n",
            "Epoch 530: Loss = 143.2494354248047\n",
            "Epoch 540: Loss = 114.01860046386719\n",
            "Epoch 550: Loss = 90.63806915283203\n",
            "Epoch 560: Loss = 71.9600830078125\n",
            "Epoch 570: Loss = 57.05986404418945\n",
            "Epoch 580: Loss = 45.18677520751953\n",
            "Epoch 590: Loss = 35.73941421508789\n",
            "Epoch 600: Loss = 28.230941772460938\n",
            "Epoch 610: Loss = 22.271713256835938\n",
            "Epoch 620: Loss = 17.54796028137207\n",
            "Epoch 630: Loss = 13.808361053466797\n",
            "Epoch 640: Loss = 10.851787567138672\n",
            "Epoch 650: Loss = 8.517046928405762\n",
            "Epoch 660: Loss = 6.676102638244629\n",
            "Epoch 670: Loss = 5.226210594177246\n",
            "Epoch 680: Loss = 4.0858612060546875\n",
            "Epoch 690: Loss = 3.1901633739471436\n",
            "Epoch 700: Loss = 2.4874279499053955\n",
            "Epoch 710: Loss = 1.9369397163391113\n",
            "Epoch 720: Loss = 1.5063122510910034\n",
            "Epoch 730: Loss = 1.1698734760284424\n",
            "Epoch 740: Loss = 0.9073461294174194\n",
            "Epoch 750: Loss = 0.7027828693389893\n",
            "Epoch 760: Loss = 0.543601393699646\n",
            "Epoch 770: Loss = 0.4198880195617676\n",
            "Epoch 780: Loss = 0.32386186718940735\n",
            "Epoch 790: Loss = 0.24945983290672302\n",
            "Epoch 800: Loss = 0.19186541438102722\n",
            "Epoch 810: Loss = 0.1473553478717804\n",
            "Epoch 820: Loss = 0.1130024641752243\n",
            "Epoch 830: Loss = 0.08653160184621811\n",
            "Epoch 840: Loss = 0.06616397202014923\n",
            "Epoch 850: Loss = 0.05051165074110031\n",
            "Epoch 860: Loss = 0.03849795088171959\n",
            "Epoch 870: Loss = 0.029301319271326065\n",
            "Epoch 880: Loss = 0.022268984466791153\n",
            "Epoch 890: Loss = 0.016897164285182953\n",
            "Epoch 900: Loss = 0.012801586650311947\n",
            "Epoch 910: Loss = 0.009685179218649864\n",
            "Epoch 920: Loss = 0.007320567034184933\n",
            "Epoch 930: Loss = 0.0055269235745072365\n",
            "Epoch 940: Loss = 0.004171167500317097\n",
            "Epoch 950: Loss = 0.003147087525576353\n",
            "Epoch 960: Loss = 0.0023749941028654575\n",
            "Epoch 970: Loss = 0.001792531693354249\n",
            "Epoch 980: Loss = 0.001355079235509038\n",
            "Epoch 990: Loss = 0.0010267493780702353\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow 환경에서 GPU 사용 유무에 따른 속도 비교\n",
        "# 런타임 - 런타임 유형 변경 - 하드웨어 가속기 - GPU 선택\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Define dimensions\n",
        "N, H, D = 64, 1000, 100\n",
        "\n",
        "# Initialize weights and input/output data\n",
        "x = tf.constant(np.random.randn(N, D), dtype=tf.float32)\n",
        "y = tf.constant(np.random.randn(N, D), dtype=tf.float32)\n",
        "w1 = tf.Variable(np.random.randn(D, H), dtype=tf.float32)\n",
        "w2 = tf.Variable(np.random.randn(H, D), dtype=tf.float32)\n",
        "\n",
        "# Define the model and loss function using tf.function for optimization\n",
        "@tf.function\n",
        "def model(x, w1, w2):\n",
        "  h = tf.maximum(tf.matmul(x, w1), 0)\n",
        "  y_pred = tf.matmul(h, w2)\n",
        "  diff = y_pred - y\n",
        "  loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
        "  return loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.optimizers.Adam()\n",
        "\n",
        "# Execute computations on GPU #1\n",
        "with tf.device('/GPU:1'):\n",
        "  # Training loop\n",
        "  for epoch in range(1000):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = model(x, w1, w2)\n",
        "      grads = tape.gradient(loss, [w1, w2])\n",
        "      optimizer.apply_gradients(zip(grads, [w1, w2]))\n",
        "      if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss.numpy()}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    }
  ]
}